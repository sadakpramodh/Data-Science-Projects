{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_1n0DRkSFr_"
      },
      "source": [
        "# Running Inference with the Mistral 7B Model\n",
        "\n",
        "In this notebook, we'll set up and utilize the Mistral 7B \"Instruct\" model. Our primary objective is to perform inference on this model and experiment with various completions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7aw9vloSFsE"
      },
      "source": [
        "### Setup Runtime\n",
        "For fine-tuning Llama, a GPU instance is essential. Follow the directions below:\n",
        "\n",
        "1. Go to `Runtime` (located in the top menu bar).\n",
        "2. Select `Change Runtime Type`.\n",
        "3. Choose `T4 GPU` (or a comparable option).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcKMzIM1SFsF"
      },
      "source": [
        "### Install Transformers Library from GitHub\n",
        "\n",
        "The code below installs the `transformers` library directly from the HuggingFace GitHub repository.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vn8qpTCHSFsF"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3KbuVSoSFsH"
      },
      "source": [
        "### Installing Additional Libraries\n",
        "\n",
        "The following commands install several libraries:\n",
        "\n",
        "- `accelerate`: A library from HuggingFace that aids in utilizing hardware accelerators like GPUs and TPUs more efficiently.\n",
        "- `bitsandbytes`: Provides fast gradient compression, beneficial for accelerated training, particularly in distributed scenarios.\n",
        "- `sentencepiece`: A library for Neural Network-based text processing, often used in tokenization processes for language models.\n",
        "\n",
        "The `-q` flag ensures a quiet installation, minimizing the log output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRtOiDE6SFsH"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft  accelerate bitsandbytes safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6VGBZYvSFsH"
      },
      "outputs": [],
      "source": [
        "!pip install sentencepiece\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgddXipySFsI"
      },
      "source": [
        "### Model Initialization and Setup\n",
        "\n",
        "In this section:\n",
        "\n",
        "- **torch**: The PyTorch library is imported, which will be used for tensor operations and to leverage GPU acceleration.\n",
        "  \n",
        "- **AutoModelForCausalLM**: From the HuggingFace Transformers library, this class provides an interface to load models designed for causal language modeling. Causal language models predict the next token in a sequence.\n",
        "\n",
        "- **AutoTokenizer**: This class is used to load tokenizers that can convert text into tokens suitable for the input of a transformer model.\n",
        "\n",
        "- `model_name`: Defines the identifier for the model we want to load. In this case, we're using the sharded version of the Mistral-7B model named [`\"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"`](https://huggingface.co/filipealmeida/Mistral-7B-Instruct-v0.1-sharded).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4noPGRC_i7u"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import transformers\n",
        "\n",
        "model_name = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2sXn-KTSFsJ"
      },
      "source": [
        "### Setting up the BitsAndBytes Configuration\n",
        "\n",
        "The code block below configures the `BitsAndBytes` quantization settings, which are designed to optimize model performance by reducing the memory requirements of the model parameters:\n",
        "\n",
        "- `load_in_4bit`: This flag, set to `True`, instructs the model to load its weights in 4-bit quantization. This can reduce memory usage significantly, allowing for larger models to fit into memory.\n",
        "\n",
        "- `bnb_4bit_use_double_quant`: When set to `True`, this flag enables double quantization, which can further enhance the efficiency of 4-bit quantization.\n",
        "\n",
        "- `bnb_4bit_quant_type`: Specifies the type of 4-bit quantization to use. The value `\"nf4\"` represents a specific form of quantization, but details on this are needed for a more complete description.\n",
        "\n",
        "- `bnb_4bit_compute_dtype`: This defines the data type to use for computations when the model weights are quantized. Here, `torch.bfloat16` is used, which is a 16-bit floating point representation that offers a balance between precision and memory usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZrNYZPOgjT1"
      },
      "outputs": [],
      "source": [
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3PMDlJRSFsJ"
      },
      "source": [
        "### Loading the Pretrained Model with Quantization\n",
        "\n",
        "The code below is responsible for loading our pretrained Mistral-7B model, utilizing the previously configured `BitsAndBytes` quantization settings:\n",
        "\n",
        "- `model_name`: Specifies the identifier for the pretrained model we want to load, which we've previously set to the sharded version of the Mistral-7B model.\n",
        "\n",
        "- `load_in_4bit`: With this set to `True`, the model loads its weights using 4-bit quantization, which significantly reduces memory requirements.\n",
        "\n",
        "- `torch_dtype`: Specifies the data type for tensor computations. We've set it to `torch.bfloat16` to strike a balance between memory efficiency and computational precision.\n",
        "\n",
        "- `quantization_config`: We provide the `BitsAndBytes` configuration (`bnb_config`) established in the previous step to apply the specified quantization settings during model loading.\n",
        "\n",
        "By leveraging these settings, the model is loaded in a memory-optimized manner, ensuring that even large models like Mistral-7B can be effectively used in constrained environments.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR6usPzNSFsJ"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    quantization_config=bnb_config\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLVs3mTNSFsK"
      },
      "source": [
        "### Tokenizer Initialization and Configuration\n",
        "\n",
        "1. **Initialize the Tokenizer**: Using the `AutoTokenizer` class from the `transformers` library, we initialize a tokenizer corresponding to our predefined model, `model_name`.\n",
        "2. **Set Beginning of Sequence Token**: The `bos_token_id` is set to `1`, designating this token ID as the beginning of a sequence.\n",
        "3. **Define Stop Tokens**: We define a list of token IDs, `stop_token_ids`, that signify stopping points in token sequences. Here, the token ID `0` is considered a stop token.\n",
        "4. **Confirmation Print**: A print statement confirms the successful loading of the model into memory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgNgHk4rd4yQ"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.bos_token_id = 1\n",
        "stop_token_ids = [0]\n",
        "\n",
        "print(f\"Successfully loaded the model {model_name} into memory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2G14SQnSFsK"
      },
      "source": [
        "### Generating Text with the Model üöÄ\n",
        "\n",
        "1. **Define Instruction Text** üìù: We set up our instruction text in the `text` variable. Remember to replace `~Add your instrunctions here~` with the actual content you wish to provide.\n",
        "2. **Tokenize Input Text** üî¢: Using our previously initialized `tokenizer`, we convert the instruction text into its tokenized form with `return_tensors=\"pt\"` to get the output as PyTorch tensors.\n",
        "3. **Model Inference** ü§ñ: With our tokenized input, we run the model's `generate` function to produce an output. We specify a maximum of 200 new tokens to be generated and enable sampling for diverse outputs.\n",
        "4. **Decode the Output** üìÑ: The generated token IDs are decoded back into human-readable text using `tokenizer.batch_decode`.\n",
        "5. **Print the Result** üñ®Ô∏è: We display the model's generated output for review.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wrw6U082SFsK"
      },
      "outputs": [],
      "source": [
        "\n",
        "text = \"[INST] ~Add your instrunctions here~ [/INST]\"\n",
        "\n",
        "encoded = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
        "model_input = encoded\n",
        "generated_ids = model.generate(**model_input, max_new_tokens=200, do_sample=True)\n",
        "decoded = tokenizer.batch_decode(generated_ids)\n",
        "print(decoded[0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}